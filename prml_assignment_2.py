# -*- coding: utf-8 -*-
"""PRML_ASSIGNMENT_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jbgaf6JZgvBBsl1xSODWe0McPpqS9l11

# General Instructions to students:

1. There are 4 types of cells in this notebook. The cell type will be indicated within the cell.
    1. Markdown cells with problem written in it. (DO NOT TOUCH THESE CELLS) (**Cell type: TextRead**)
    2. Python cells with setup code for further evaluations. (DO NOT TOUCH THESE CELLS) (**Cell type: CodeRead**)
    3. Python code cells with some template code or empty cell. (FILL CODE IN THESE CELLS BASED ON INSTRUCTIONS IN CURRENT AND PREVIOUS CELLS) (**Cell type: CodeWrite**)
    4. Markdown cells where a written reasoning or conclusion is expected. (WRITE SENTENCES IN THESE CELLS) (**Cell type: TextWrite**)
    
2. You are not allowed to insert new cells in the submitted notebook.

3. You are not allowed to import any extra packages, unless needed.

4. The code is to be written in Python 3.x syntax. Latest versions of other packages maybe assumed.

5. In CodeWrite Cells, the only outputs to be given are plots asked in the question. Nothing else to be output/printed.

6. If TextWrite cells ask you to give accuracy/error/other numbers, you can print them on the code cells, but remove the print statements before submitting.

7. Any runtime failures on the submitted notebook will get zero marks.

8. All code must be written by you. Copying from other students/material on the web is strictly prohibited. Any violations will result in zero marks.

10. All plots must be labelled properly, the labels/legends should be readable, all tables must have rows and columns named properly.

11. Change the name of file with your roll no. For example cs15d203.ipynb (for notebook) and cs15d203.py (for plain python script)
"""

# Cell type : CodeRead

import numpy as np
import matplotlib.pyplot as plt

"""
**Cell type : TextRead**

**Question 6: Coding Life in Lower Dimensions**

You are provided with a [dataset](https://drive.google.com/file/d/1wOp8K9BS8Ncmjz7aP8RdqukqkGNSwK4i/view?usp=drive_link) of 1797 images - each image is 8x8 pixels and provided as a feature vector of length 64. You will try your hands at transforming this dataset to a lower-dimensional space using PCA.

**6a)** Code up and run the PCA algorithm on the given dataset. Plot the cumulative percentage variance explained by the principal components. Report the minimum number of principal components that contribute to at least 90% of the variance in the dataset.

**Note: Write the code from scratch for PCA. The only exception** is the computation of eigenvalues and eigenvectors for which you could use the numpy in-bulit function (specifically, do NOT use other functions like numpy.cov). If you have doubts about whether a particular library function can be used, please ask in Ed Discussion and we will clarify."""

# # Cell type : CodeWrite
# # Write in this cell any helper/utility functions you may need for the perform_PCA function below.
# # Some sample functions are given here as recommendations, however feel free to write your own.
# def compute_cov(Data):
#     # your code here (use 1/N factor and not 1/(N-1) in sample covariance calcn.)
#     return sampleCovarMatrix

# def compute_mean(Data):
#     # your code here
#     return colMeans

# # add other helper/util fns. as needed




def find_mean(x):
    no_of_rows=len(x)
    no_of_columns=len(x[0])

    mean=[]    #  1 x no_of-columns
    for i in range(no_of_columns):
        col_sum=0
        for j in range(no_of_rows):
            col_sum=col_sum+x[j][i]
        mean.append(col_sum/no_of_rows)

    return mean





def find_centered_mean(x,mean_data):

    no_of_rows=len(x)
    no_of_columns=len(x[0])
    inter_result=[]
    centered_mean=[]     # no_of_rows x no_of_columns

    for i in range(no_of_rows):
        row=[]
        for j in range(no_of_columns):
            row.append(0)
        centered_mean.append(row)

    for i in x:
        inter_result.append(i)

    for i in range(no_of_rows):
        for j in range(no_of_columns):
            centered_mean[i][j]=inter_result[i][j]-mean_data[j]

    return centered_mean



def find_covariance(x,data):

    no_of_rows=len(x)
    no_of_columns=len(x[0])
    transpose=[]
    cov_matrix=[]      # no_of_columns x no_of columns

    for i in range(no_of_columns):
        row=[]
        for j in range(no_of_rows):
            row.append(0)
        transpose.append(row)

    for i in range(no_of_columns):
        row=[]
        for j in range(no_of_columns):
            row.append(0)
        cov_matrix.append(row)

    for i in range(no_of_rows):
        for j in range(no_of_columns):
            transpose[j][i]=data[i][j]


    for i in range(no_of_columns):
        for j in range(no_of_columns):
            for k in range(no_of_rows):
                cov_matrix[i][j]=cov_matrix[i][j]+transpose[i][k]*data[k][j]


    for i in range(no_of_columns):
        for j in range(no_of_columns):
            cov_matrix[i][j]=cov_matrix[i][j]/no_of_rows


    return cov_matrix

# # Cell type : CodeWrite
# # Write the function for PCA here.
# def perform_PCA(Data, M=None):
#     """ Perform PCA on the input data matrix
#         Feel free to modify the function arguments and outputs to your needs -- what we've provided here is just our recommended function signature, which you can ovverride.
#         Also feel free to use any helper/utility functions from the previous cell.

#     Arguments:
#         Data: dataset of shape N x D (n_samples x n_features)
#         M: (aka n_PCs) focus on only the top M PCs (Principal Components).
#             Should satisfy: 1 <= M <= D.
#             If M=None, then we set M=D (i.e., focus on all the PCs).

#     Returns:
#         dimreduced_D: transformed (dimensionality-reduced) data of shape N x M (n_samples x n_PCs)
#         selPCs: top M selected PCs in a matrix of shape D x M (n_features x n_PCs)
#         explained_variances: amount of variance explained by each of the selPCs of shape M x 1 (n_PCs x 1)
#                              (these are the eigenvalues of the sample covar. matrix, sorted in non-ascending order)
#     """
#     #your code here
#     #ensure that explained_variances (eigenvalues) are sorted in descending order
#     return dimreduced_D, selPCs, explained_variances

# def plot_cumulative_percent_variance(explained_variances):
#     # your code here to plot the # of chosen top PCs along the x-axis and cumulative percentage variance explained by these chosen PCs along the y-axis
#     # your code to use the computed cumulative pct. variance to report the min # of PCs contributing to at least 90% of the total variance in the data
#     return






def perform_PCA(x):


    mean=find_mean(x)
    centered_mean=find_centered_mean(x,mean)
    data=find_covariance(x,centered_mean)

    no_of_rows=len(x)
    no_of_columns=len(x[0])

    eigenvalues,eigenvectors=np.linalg.eig(data)

    values=eigenvalues.copy()
    vectors=eigenvectors.copy()
    for i in range(len(values)):
        for j in range(0,len(values)-i-1):
            if values[j]<values[j+1]:
                values[j],values[j+1]=values[j+1],values[j]
                temp=vectors[:,j].copy()
                vectors[:,j]=vectors[:,j+1]
                vectors[:,j+1]=temp


    return values,vectors

def plot_cumulative_percent_variance(values):
    sum_variance=0
    for i in values:
        sum_variance+=i

    individual_variance=values/sum_variance


    variance=[]
    a=0
    for i in values:
        a=a+i
        variance.append(a/sum_variance)

    fin_variance=[]
    for i in variance:
        fin_variance.append(i*100)

    comp=0
    for i in range(len(variance)):
        if variance[i]>=0.9:
            comp=i+1
            break

    plt.figure(figsize=(12,6))
    plt.plot(range(1,len(fin_variance)+1),fin_variance,color='blue',marker='o',markerfacecolor='red',linewidth=2,label='Cumulative Variance')
    plt.scatter(comp,fin_variance[i],color='green',s=100,zorder=5,label=f'Minimum number of components that compute atleast 90% of variance is {comp}')
    plt.grid(True)
    plt.xlabel('Principal Components')
    plt.ylabel('Cummulative Variance %')
    plt.title('Principal Components and their Cumulative Variance')
    plt.legend()
    plt.show()


data=np.load(r"C:\Users\palip\OneDrive\Desktop\Data.npz")
x=data['arr_0']
values,vectors=perform_PCA(x)
plot_cumulative_percent_variance(values)

"""**Cell type : TextRead**

**6b)**  Perform reconstruction of data using the dimensionality-reduced data considering the number of dimensions $M \in \{2,4,8,16\}$. Report the Mean Square Error (MSE) between the original data and reconstructed data, and interpret the optimal dimensions $\hat{d}$ based on the MSE values.
"""

# Cell type : CodeWrite
# Write the code for data reconstruction, and run the algorithm for selecting the number of dimensions.


def find_reconstruct(x,val):

    mean=find_mean(x)
    centered_mean=find_centered_mean(x,mean)
    #data=find_covariance(x,fin_result)
    no_of_rows=len(x)
    no_of_columns=len(x[0])
    values,vectors=perform_PCA(x)


    x_mean=centered_mean    # no_of_rows x no_of_columns
    eigen_vect=vectors[:,:val]  # no_of_columns x val



    z=[]   # no_of_rows x val
    for i in range(no_of_rows):
        row=[]
        for j in range(val):
            row.append(0)
        z.append(row)


    transpose=[]     #val x no_of_columns
    for i in range(val):
        row=[]
        for j in range(no_of_columns):
            row.append(0)
        transpose.append(row)


    for i in range(no_of_columns):
        for j in range(val):
            transpose[j][i]=eigen_vect[i][j]


    for i in range(no_of_rows):
        for j in range(val):
            for k in range(no_of_columns):
                z[i][j]=z[i][j]+x_mean[i][k]*eigen_vect[k][j]
    #print(z)



    reconstruct=[]
    for i in range(no_of_rows):
        row=[]
        for j in range(no_of_columns):
            row.append(0)
        reconstruct.append(row)



    for i in range(no_of_rows):
        for j in range(no_of_columns):
            for k in range(val):
                reconstruct[i][j]=reconstruct[i][j]+z[i][k]*transpose[k][j]

    for i in range(no_of_rows):
        for j in range(no_of_columns):
            reconstruct[i][j]=reconstruct[i][j]+mean[j]

   # print(reconstruct)

    mse=0
    for i in range(no_of_rows):
        for j in range(no_of_columns):
            mse=mse+(x[i][j]-reconstruct[i][j])**2
    mse=mse/(no_of_rows)
    #print(mse)

    return mse


M=[2,4,8,16]
print(f"{'M value':^15}{'MSE Value':^15}")
print()

for i in M:
    error=find_reconstruct(x,i)
    print(f"{i:^15}{error.real:^15.6f}")

"""
**Cell type : TextRead**

**6c)** Let's now apply the same PCA code that you've written above to analyze images to understand text. Large language models (LLMs) typically analyze text by representing words as vectors, also known as embeddings. You are provided with 768-dimensional embeddings (extracted from a LLM called BERT) of 10 words in the [folder](https://drive.google.com/drive/folders/1yshmnkWoALf2ZwqgZqWwXJk2W5_XuCLG?usp=sharing).

Apply your PCA code with $M=2$ PCs on these embeddings to visualize the 10 words in 2-D. Report how much percentage of variation is captured by these two PCs. What does this (2D scatterplot) visualization tell you about the embeddings of related vs. unrelated words?

(Note: If applying the same PCA code to this embeddings data matrix $X$ takes a long time, then consider applying it to $X^T$ and use the returned PCs to derive the PCs of $X$ as in question Q3 of the Assignment.)"""

# Cell type : CodeWrite
# Write your code here as instructed.
# (Use the function(s) written previously)





def find_clusters(x):
    mean=find_mean(x)
    centered_mean=find_centered_mean(x,mean)
    values,vectors=perform_PCA(x)

    no_of_rows=len(x)
    no_of_columns=len(x[0])

    sum_variance=0
    for i in values:
        sum_variance+=i

    variance_of_pc=values[0]+values[1]
    percent=(variance_of_pc/sum_variance)*100
    #print("Percentage of total variance explained by PC1 and PC2 is ",percent.real)

    x_mean=centered_mean
    eigen_vect=vectors[:,:2]
    z=[]
    for i in range(no_of_rows):
        row=[]
        for j in range(2):
            row.append(0)
        z.append(row)


    for i in range(no_of_rows):
        for j in range(2):
            for k in range(no_of_columns):
                z[i][j]=z[i][j]+x_mean[i][k]*eigen_vect[k][j]

    plt.figure(figsize=(10,5))

    tit1=f"PCA of Word Embeddings"
    tit2=f"(Percentage of total variance explained by PC1 and PC2 is : {percent.real:.5f}%)"

    for i in range(no_of_rows):
        plt.scatter(z[i][0],z[i][1],color='red')
        plt.text(z[i][0]+0.01,z[i][1]+0.01,words[i],fontsize=10)
        #plt.plot(label=f'Cumulative Variance{percent.real}')

    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.title(f"{tit1}\n{tit2}")
    plt.grid(True)
    plt.show()


words=[]
embeddings=[]
with open(r"C:\Users\palip\OneDrive\Desktop\prml_asst_2_word_embeddings.txt","r")as f:
    for i in f:
        p=i.strip().split()
        words.append(p[0])
        values=[]
        for x in p[1:]:
            values.append(float(x))
        embeddings.append(values)

y=embeddings
find_clusters(y)

